{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9a68d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "#webscraping\n",
    "import time, os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver import ChromeOptions,Chrome\n",
    "from bs4 import BeautifulSoup\n",
    "from undetected_chromedriver import ChromeOptions\n",
    "import requests\n",
    "import undetected_chromedriver as uc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d79e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attraction_images(URL):\n",
    "    \n",
    "    error = 0\n",
    "     # Set up the Selenium WebDriver\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"window-size=1200x800\")\n",
    "    options.add_argument('--user-data-dir=/Users/yasirutj/Library/Application Support/Google/Chrome/Profile 1')\n",
    "    driver = uc.Chrome(options=options)\n",
    "    driver.implicitly_wait(20)\n",
    "    \n",
    "    driver.get(URL)\n",
    "    \n",
    "    try:\n",
    "        # Click \"I Accept\" button that shows up when the web page is first loaded\n",
    "        i_accept = driver.find_element(\"id\", \"onetrust-accept-btn-handler\").click()\n",
    "    except:\n",
    "        error + 1\n",
    "    \n",
    "    try:\n",
    "        address_element = driver.find_element(By.XPATH, \"//button[@class='UikNM _G B- _S _W _T c G_ wSSLS wnNQG raEkE']\")#biGQs _P XWJSj Wb\n",
    "        # print(address_element)\n",
    "        address = address_element.text\n",
    "    except:\n",
    "        address = \"\"\n",
    "    \n",
    "    try:\n",
    "        # click All photos button\n",
    "        # use double quotes inside contains(text(), \"All photos\")\n",
    "        all_photos_button = driver.find_element(By.XPATH, \"//*[contains(text(), 'All photos')]\")\n",
    "        all_photos_button.click()\n",
    "    except:\n",
    "        error + 1\n",
    "   \n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, \"lxml\")\n",
    "    time.sleep(3)\n",
    "    \n",
    "    images = soup.find_all('img') #find all imgs\n",
    "    \n",
    "    #loop through imgs, check for 2 image types, add to list if it is a TripAdvisor user uploaded photo\n",
    "    img_links = []\n",
    "    errors = 0\n",
    "    for i in images:\n",
    "        try:\n",
    "                link = i.attrs['src']\n",
    "                if re.search('photo',link):\n",
    "                    img_links.append(link)\n",
    "        except:\n",
    "            errors = errors + 1\n",
    "            \n",
    "            \n",
    "    print(img_links)\n",
    "    print(len(img_links))\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    return address, img_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af882f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attraction_base(url):\n",
    "    \n",
    "    attraction_df = pd.DataFrame(columns=['name', 'location', 'state', 'img_num'])\n",
    "    idx = 1\n",
    "    error = 0\n",
    "    img_link_dict = {}\n",
    "    \n",
    "    if url == 'https://www.tripadvisor.co.uk/Attractions-g186217-Activities-oa0-England.html' :\n",
    "        state = 'England'\n",
    "    elif url == 'https://www.tripadvisor.co.uk/Attractions-g186425-Activities-oa0-Wales.html':\n",
    "        state = 'Wales'\n",
    "    elif url == 'https://www.tripadvisor.co.uk/Attractions-g186485-Activities-oa0-Scotland.html':\n",
    "        state = 'Scotland'\n",
    "    elif url == 'https://www.tripadvisor.co.uk/Attractions-g186469-Activities-oa0-Northern_Ireland.html':\n",
    "        state = 'Nothern Ireland'\n",
    "    \n",
    "    # Set up the Selenium WebDriver\n",
    "    options = uc.ChromeOptions()\n",
    "    driver = uc.Chrome(options=options)\n",
    "    driver.implicitly_wait(20)  # Adjust the wait time as needed\n",
    "\n",
    "    index = 0\n",
    "    # Specify the URL for the 4 different states in UK\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for any initial popups to load\n",
    "    time.sleep(5)\n",
    "\n",
    "    try:\n",
    "        # Click \"I Accept\" button that shows up when the web page is first loaded\n",
    "        i_accept = driver.find_element(\"id\", \"onetrust-accept-btn-handler\").click()\n",
    "    except:\n",
    "        error + 1\n",
    "\n",
    "    # Initialize BeautifulSoup with the current page source\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, \"lxml\")\n",
    "\n",
    "    url = \"https://www.tripadvisor.com\"\n",
    "    links = {}\n",
    "    body = soup.find_all('body')[0]\n",
    "    link_list = body.find_all( 'a') #find all links on page\n",
    "        \n",
    "    #loop through range to get numbers 1-30, we want top 30 attractions of each state, then for each number\n",
    "    #loop through links, add attraction name and link to links dictionary if number match outside loop\n",
    "    for x in range(1,31):\n",
    "            find = str(x) + \"[.] \"\n",
    "            for l in link_list:\n",
    "                if re.search(find,l.text):\n",
    "                    name = l.text\n",
    "                    name = re.sub(r'^.*? ', '', name) #clean name\n",
    "                    links[name] = url + l['href']\n",
    "    \n",
    "    next_page_btn = driver.find_element(By.XPATH, \"//a[@class='BrOJk u j z _F wSSLS tIqAi unMkR']\") #BrOJk u j z _F wSSLS tIqAi unMkR\n",
    "    next_page_btn.click()\n",
    "    time.sleep(10)\n",
    "\n",
    "    try:\n",
    "        # Click \"I Accept\" button that shows up when the web page is first loaded\n",
    "        i_accept = driver.find_element(\"id\", \"onetrust-accept-btn-handler\").click()\n",
    "    except:\n",
    "        error + 1\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source) # specify lxml? not specified cus beautifoulsoup by default finds the parser that best suits our data type\n",
    "    body = soup.find_all('body')[0]\n",
    "    link_list = body.find_all( 'a') #find all links on page\n",
    "\n",
    "    #loop through range to get numbers 31-60,\n",
    "    for x in range(31,61): \n",
    "        find = re.compile(fr\"{x}\\. \")  # Using regex for the pattern\n",
    "        for l in link_list:\n",
    "            if find.search(l.text):\n",
    "                name = re.sub(r'^.*? ', '', l.text)  # Clean name\n",
    "                links[name] = url + l.get('href') \n",
    "                \n",
    "    next_page_btn = driver.find_element(By.CSS_SELECTOR, 'a[aria-label=\"Next page\"]') #BrOJk u j z _F wSSLS tIqAi iNBVo\n",
    "    next_page_btn.click()\n",
    "    time.sleep(10)\n",
    "\n",
    "    try:\n",
    "        # Click \"I Accept\" button that shows up when the web page is first loaded\n",
    "        i_accept = driver.find_element(\"id\", \"onetrust-accept-btn-handler\").click()\n",
    "    except:\n",
    "        error + 1\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source) # specify lxml? not specified cus beautifoulsoup by default finds the parser that best suits our data type\n",
    "    body = soup.find_all('body')[0]\n",
    "    link_list = body.find_all( 'a') #find all links on page\n",
    "\n",
    "    #loop through range to get numbers 61-90,\n",
    "    for x in range(61,90): \n",
    "        find = re.compile(fr\"{x}\\. \")  # Using regex for the pattern\n",
    "        for l in link_list:\n",
    "            if find.search(l.text):\n",
    "                name = re.sub(r'^.*? ', '', l.text)  # Clean name\n",
    "                links[name] = url + l.get('href') \n",
    "                \n",
    "                \n",
    "    next_page_btn = driver.find_element(By.CSS_SELECTOR, 'a[aria-label=\"Next page\"]') #BrOJk u j z _F wSSLS tIqAi unMkR\n",
    "    next_page_btn.click()\n",
    "    time.sleep(10)\n",
    "\n",
    "    try:\n",
    "        # Click \"I Accept\" button that shows up when the web page is first loaded\n",
    "        i_accept = driver.find_element(\"id\", \"onetrust-accept-btn-handler\").click()\n",
    "    except:\n",
    "        error + 1\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source) # specify lxml? not specified cus beautifoulsoup by default finds the parser that best suits our data type\n",
    "    body = soup.find_all('body')[0]\n",
    "    link_list = body.find_all( 'a') #find all links on page\n",
    "\n",
    "    #loop through range to get numbers 91-120,\n",
    "    for x in range(91,120): \n",
    "        find = re.compile(fr\"{x}\\. \")  # Using regex for the pattern\n",
    "        for l in link_list:\n",
    "            if find.search(l.text):\n",
    "                name = re.sub(r'^.*? ', '', l.text)  # Clean name\n",
    "                links[name] = url + l.get('href') \n",
    "                \n",
    "        next_page_btn = driver.find_element(By.CSS_SELECTOR, 'a[aria-label=\"Next page\"]') #BrOJk u j z _F wSSLS tIqAi unMkR\n",
    "    next_page_btn.click()\n",
    "    time.sleep(10)\n",
    "\n",
    "    try:\n",
    "        # Click \"I Accept\" button that shows up when the web page is first loaded\n",
    "        i_accept = driver.find_element(\"id\", \"onetrust-accept-btn-handler\").click()\n",
    "    except:\n",
    "        error + 1\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source) # specify lxml? not specified cus beautifoulsoup by default finds the parser that best suits our data type\n",
    "    body = soup.find_all('body')[0]\n",
    "    link_list = body.find_all( 'a') #find all links on page\n",
    "\n",
    "    #loop through range to get numbers 121-150,\n",
    "    for x in range(121,150): \n",
    "        find = re.compile(fr\"{x}\\. \")  # Using regex for the pattern\n",
    "        for l in link_list:\n",
    "            if find.search(l.text):\n",
    "                name = re.sub(r'^.*? ', '', l.text)  # Clean name\n",
    "                links[name] = url + l.get('href') \n",
    "    \n",
    "        next_page_btn = driver.find_element(By.CSS_SELECTOR, 'a[aria-label=\"Next page\"]') #BrOJk u j z _F wSSLS tIqAi unMkR\n",
    "    next_page_btn.click()\n",
    "    time.sleep(10)\n",
    "\n",
    "    try:\n",
    "        # Click \"I Accept\" button that shows up when the web page is first loaded\n",
    "        i_accept = driver.find_element(\"id\", \"onetrust-accept-btn-handler\").click()\n",
    "    except:\n",
    "        error + 1\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source) # specify lxml? not specified cus beautifoulsoup by default finds the parser that best suits our data type\n",
    "    body = soup.find_all('body')[0]\n",
    "    link_list = body.find_all( 'a') #find all links on page\n",
    "\n",
    "    #loop through range to get numbers 151-180,\n",
    "    for x in range(151,180): \n",
    "        find = re.compile(fr\"{x}\\. \")  # Using regex for the pattern\n",
    "        for l in link_list:\n",
    "            if find.search(l.text):\n",
    "                name = re.sub(r'^.*? ', '', l.text)  # Clean name\n",
    "                links[name] = url + l.get('href') \n",
    "                \n",
    "        next_page_btn = driver.find_element(By.CSS_SELECTOR, 'a[aria-label=\"Next page\"]') #BrOJk u j z _F wSSLS tIqAi unMkR\n",
    "    next_page_btn.click()\n",
    "    time.sleep(10)\n",
    "\n",
    "    try:\n",
    "        # Click \"I Accept\" button that shows up when the web page is first loaded\n",
    "        i_accept = driver.find_element(\"id\", \"onetrust-accept-btn-handler\").click()\n",
    "    except:\n",
    "        error + 1\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source) # specify lxml? not specified cus beautifoulsoup by default finds the parser that best suits our data type\n",
    "    body = soup.find_all('body')[0]\n",
    "    link_list = body.find_all( 'a') #find all links on page\n",
    "\n",
    "    #loop through range to get numbers 181-210,\n",
    "    for x in range(181,210): \n",
    "        find = re.compile(fr\"{x}\\. \")  # Using regex for the pattern\n",
    "        for l in link_list:\n",
    "            if find.search(l.text):\n",
    "                name = re.sub(r'^.*? ', '', l.text)  # Clean name\n",
    "                links[name] = url + l.get('href') \n",
    "                \n",
    "        next_page_btn = driver.find_element(By.CSS_SELECTOR, 'a[aria-label=\"Next page\"]') #BrOJk u j z _F wSSLS tIqAi unMkR\n",
    "    next_page_btn.click()\n",
    "    time.sleep(10)\n",
    "\n",
    "    try:\n",
    "        # Click \"I Accept\" button that shows up when the web page is first loaded\n",
    "        i_accept = driver.find_element(\"id\", \"onetrust-accept-btn-handler\").click()\n",
    "    except:\n",
    "        error + 1\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source) # specify lxml? not specified cus beautifoulsoup by default finds the parser that best suits our data type\n",
    "    body = soup.find_all('body')[0]\n",
    "    link_list = body.find_all( 'a') #find all links on page\n",
    "\n",
    "    #loop through range to get numbers 211-240,\n",
    "    for x in range(211,240): \n",
    "        find = re.compile(fr\"{x}\\. \")  # Using regex for the pattern\n",
    "        for l in link_list:\n",
    "            if find.search(l.text):\n",
    "                name = re.sub(r'^.*? ', '', l.text)  # Clean name\n",
    "                links[name] = url + l.get('href') \n",
    "                \n",
    "    next_page_btn = driver.find_element(By.CSS_SELECTOR, 'a[aria-label=\"Next page\"]') #BrOJk u j z _F wSSLS tIqAi unMkR\n",
    "    next_page_btn.click()\n",
    "    time.sleep(10)\n",
    "\n",
    "    try:\n",
    "        # Click \"I Accept\" button that shows up when the web page is first loaded\n",
    "        i_accept = driver.find_element(\"id\", \"onetrust-accept-btn-handler\").click()\n",
    "    except:\n",
    "        error + 1\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source) # specify lxml? not specified cus beautifoulsoup by default finds the parser that best suits our data type\n",
    "    body = soup.find_all('body')[0]\n",
    "    link_list = body.find_all( 'a') #find all links on page\n",
    "\n",
    "    #loop through range to get numbers 241-270,\n",
    "    for x in range(241,270): \n",
    "        find = re.compile(fr\"{x}\\. \")  # Using regex for the pattern\n",
    "        for l in link_list:\n",
    "            if find.search(l.text):\n",
    "                name = re.sub(r'^.*? ', '', l.text)  # Clean name\n",
    "                links[name] = url + l.get('href') \n",
    "                \n",
    "        next_page_btn = driver.find_element(By.CSS_SELECTOR, 'a[aria-label=\"Next page\"]') #BrOJk u j z _F wSSLS tIqAi unMkR\n",
    "    next_page_btn.click()\n",
    "    time.sleep(10)\n",
    "\n",
    "    try:\n",
    "        # Click \"I Accept\" button that shows up when the web page is first loaded\n",
    "        i_accept = driver.find_element(\"id\", \"onetrust-accept-btn-handler\").click()\n",
    "    except:\n",
    "        error + 1\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source) # specify lxml? not specified cus beautifoulsoup by default finds the parser that best suits our data type\n",
    "    body = soup.find_all('body')[0]\n",
    "    link_list = body.find_all( 'a') #find all links on page\n",
    "\n",
    "    #loop through range to get numbers 91-120,\n",
    "    for x in range(271,300): \n",
    "        find = re.compile(fr\"{x}\\. \")  # Using regex for the pattern\n",
    "        for l in link_list:\n",
    "            if find.search(l.text):\n",
    "                name = re.sub(r'^.*? ', '', l.text)  # Clean name\n",
    "                links[name] = url + l.get('href') \n",
    "\n",
    "    for name, link in links.items():\n",
    "        address, img_links = attraction_images(link)\n",
    "        img_link_dict[name] = img_links\n",
    "        new_row = pd.DataFrame({'name':name, 'location': address, 'state': state , 'img_num' : len(img_links)}, index=[idx])\n",
    "        idx += 1\n",
    "        attraction_df = pd.concat ([attraction_df,new_row])\n",
    "        \n",
    "    # Close the WebDriver   \n",
    "    driver.quit()\n",
    "    \n",
    "    return (attraction_df,img_link_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9ba799",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_df = pd.DataFrame(columns = ['name', 'location', 'img_num'])\n",
    "img_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9df4659",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['https://www.tripadvisor.co.uk/Attractions-g186217-Activities-oa0-England.html', 'https://www.tripadvisor.co.uk/Attractions-g186425-Activities-oa0-Wales.html', 'https://www.tripadvisor.co.uk/Attractions-g186485-Activities-oa0-Scotland.html', 'https://www.tripadvisor.co.uk/Attractions-g186469-Activities-oa0-Northern_Ireland.html']\n",
    "\n",
    "for url in states:\n",
    "    attraction_dataframe, images = attraction_base(url)\n",
    "\n",
    "    loc_df = pd.concat([loc_df,attraction_dataframe], axis=0, ignore_index=True)\n",
    "    img_dict.update(images)\n",
    "\n",
    "loc_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeb74e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_df.to_pickle('attractions_data_all.pkl')\n",
    "\n",
    "loc_df.to_csv(\"attractions_data_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dataframe = pd.DataFrame.from_dict(img_dict, orient= 'index')\n",
    "img_dataframe.to_pickle('attractions_image_links_all.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
